<pre class='metadata'>
Title: `std::execution::bulk()` issues
Shortname: D3481
Level: 4
Status: P
Group: wg21
Editor: Lucian Radu Teodorescu, Garmin, lucteo@lucteo.ro
        Ruslan Arutyunyan, Intel, ruslan.arutyunyan@intel.com
        Lewis Baker, lewissbaker@gmail.com
        Mark Hoemmen, NVIDIA, mhoemmen@nvidia.com
Audience: SG1, LEWG, LWG
URL: http://wg21.link/P3481R4
Abstract: This paper explores the issues with `std::execution::bulk` algorithm and proposes a way to address them,
          making the API better for the users.
Markup Shorthands: markdown yes
</pre>

<style>
div.ed-note, span.ed-note {
  color: light-dark(blue, cornflowerblue) !important;
  font: initial;
  font-family: sans-serif;
}
div.ed-note:before, span.ed-note:before {
  content: "[Editorial note: ";
  font-style: italic;
}
div.ed-note:after, span.ed-note:after {
  content: " — end note]";
  font-style: italic;
}
.ins, ins, ins *, span.ins, span.ins * {
  background-color: light-dark(#cfc, hsl(120, 100.00%, 15%));
  color: light-dark(#000, #ddd);
  text-decoration: none;
}
.del, del, del *, span.del, span.del * {
  background-color: light-dark(#fcc,hsl(0, 100.00%, 20%));
  color: light-dark(#000, #ddd);
  text-decoration: line-through;
  text-decoration-color: #c00;
}
div.ed-note *, span.ed-note * {
  color: blue !important;
  margin-top: 0em;
  margin-bottom: 0em;
}
div.ed-note blockquote {
  margin-left: 2em;
}
div.block-insert, div.block-insert {
  background-color: light-dark(#cfc, hsl(120, 100.00%, 15%));
  padding-left: 0;
  padding-right: 1.6em;
}
div.block-insert pre.highlight {
  background-color: transparent;
}
</style>

# Changes history # {#history}

## R4 ## {#r4}
- Improve wording.
- Add execution policy to `bulk_unchunked`.
- remove extra check in `bulk_unchunked` for concurrent schedulers

## R3 ## {#r3}
- Improve wording.

## R2 ## {#r2}
- Incorporate to the feedback from SG1 review, Austria, 2025.
    - Incorporate the changes from [[P3564R0]]: Make calling un-customized `bulk_unchunked` ill-formed on a scheduler with concurrent forward progress.
    - Remove `bulk_unchunked`'s execution policy parameter.
    - Specify an error for the "lack of resources" case.

## R1 ## {#r1}

- Incorporate to the feedback from SG1 review, Poland, 2024.
- Add wording section.

# Motivation # {#motivation}

When [[P2300R10]] was merged into the working draft, a subsequent review of the github issue tracker discovered several
outstanding issues relating to the use of the `bulk` algorithm that was in [[P2300R10]] which were not addressed during the
LEWG review. These issues mostly account for better expressing what `bulk` can and cannot do.

For most algorithms defined in [[P2300R10]], the default implementation typically describes the desired behavior.
However, this is not the case for `bulk`.

The expectation for `bulk` is that it will be customized for most use cases.
The default implementation is a common-denominator solution that is not necessarily optimal for the majority of scenarios.

The description in [[exec.bulk]] specifies the default implementation but does not outline any requirements for customizations.
This creates uncertainty for users regarding what they can expect when invoking `bulk`.

This paper addresses this issue by specifying what is permissible in a customization of `bulk`.

A second issue this paper seeks to resolve is the lack of an execution policy for the provided functor.
For example, imagine that we have scheduler that supports `par_unseq` execution policy only for parallelism (certain GPU
schedulers) and it wants to customize `bulk`. If the functor is unsuitable for the `par_unseq` execution policy, there is
currently no mechanism for users to express this with the current API shape.

A third issue this paper addresses is the absence of chunking in the default implementation of `bulk`.
Invoking the functor for every element in a range can lead to performance challenges in certain use cases.


## Allowed behavior for `bulk` customizations ## {#motivation.allowed}

From the specification of bulk in [[exec.bulk]], if we have customized `bulk`, the following are unclear:
- Can `bulk` invoke the given functor concurrently?
- Can `bulk` create decayed copies of the given functor?
- Can `bulk` create decayed copies of the values produced by the previous sender?
- Can `bulk` handle cancellation within the algorithm?
- How should `bulk` respond to exceptions thrown by the given functor?

## Execution policy for the given functor ## {#motivation.policy}

Similar to `for_each` ([[alg.foreach]]) users might expect the `bulk` algorithm to take an execution policy as argument.
The execution policy would define the execution capabilities of the given function.

There are two main reasons for wanting an execution policy to accompany the function passed to `bulk`:
1. To better tailor the algorithm to the available hardware.
2. To improve usage in generic code.

Currently, we have hardware that supports both `par` and `par_unseq` execution policy semantics, as well as hardware that only supports `par_unseq`.
Standardizing the API without execution policies implies that the language chooses a default execution policy for `bulk`.
This decision could favor certain hardware vendors over others.

From a generic programming perspective, consider a scenario where a user implements a function like the following:

```cpp
void process(std::vector<int>& data, auto std::invocable<int> f) {
  // call `f` for each element in `data`; try using `bulk`.
  // can we call `f` concurrently?
}
```

In the body of the generic function process, we do not know the constraints imposed on `f`.
Can we assume that `f` can be invoked with the `par_unseq` execution policy?
Can it be invoked with the `par` execution policy?
Or should we default to the `seq` execution policy?

Since there is no way to infer the execution policy that should apply to `f`, the implementation of process must be conservative and default to the `seq` execution policy.
However, in most cases, the `par` or `par_unseq` execution policy would likely be more appropriate.


## Chunking ## {#motivation.chunking}

The current specification of `bulk` ([[exec.bulk]]) does not support chunking.
This limitation presents a performance issue for certain use cases.

If two iterations can potentially be executed together more efficiently than in isolation, chunking would provide a performance benefit.

Let us take an example:

```cpp
std::atomic<std::uint32_t> sum{0};
std::vector<std::uint32_t> data;
ex::sender auto s = ex::bulk(ex::just(), std::execution::par, 100'000,
  [&sum,&data](std::uint32_t idx) {
    sum.fetch_add(data[idx]);
  });
```

In this example, we perform 100,000 atomic operations.
A more efficient implementation would allow a single atomic operation for each chunk of work, enabling each chunk to perform local summation. This approach might look like:

```cpp
std::atomic<std::uint32_t> sum{0};
ex::sender auto s = ex::bulk_chunked(ex::just(), std::execution::par, 100'000,
  [&sum,&data](std::uint32_t begin, std::uint32_t end) {
    std::uint32_t partial_sum = 0;
    while (begin != end) {
      partial_sum += data[idx];
    }
    sum.fetch_add(partial_sum);
  });
```

Other similar examples exist where grouping operations together can improve performance.
This is particularly important when the functor passed to `bulk` cannot be inlined or when the functor is expensive to invoke.


# API # {#api}

Define `bulk` to match the following API:

```cpp
// NEW: algorithm to be used as a default basis operation
template<execution::sender Predecessor,
         typename ExecutionPolicy,
         std::integral Size,
         std::invocable<Size, Size, values-sent-by(Predecessor)...> Func
>
execution::sender auto bulk_chunked(Predecessor pred,
                                    ExecutionPolicy&& pol,
                                    Size size,
                                    Func f);

// NEW: algorithm to be used when one execution agent per iteration is desired
template<execution::sender Predecessor,
         typename ExecutionPolicy,
         std::integral Size,
         std::invocable<Size, values-sent-by(Predecessor)...> Func
>
execution::sender auto bulk_unchunked(Predecessor pred,
                                      ExecutionPolicy&& pol,
                                      Size size,
                                      Func f);

template<execution::sender Predecessor,
         typename ExecutionPolicy,
         std::integral Size,
         std::invocable<Size, values-sent-by(Predecessor)...> Func
>
execution::sender auto bulk(Predecessor pred,
                            ExecutionPolicy&& pol, // NEW
                            Size size,
                            Func f) {
    // Default implementation
    return bulk_chunked(
        std::move(pred),
        std::forward<ExecutionPolicy>(pol),
        size,
        [func=std::move(func)]<typename... Vs>(
            Size begin, Size end, Vs&&... vs)
            noexcept(std::is_nothrow_invocable_v<Func, Size, Vs&&...>) {
            while (begin != end) {
                f(begin++, std::forward<Vs>(vs)...);
            }
        });
}
```


# Design discussions # {#design}

## Use chunked version as a basis operation ## {#design.chunked}

To address the performance problem described in the motivation, we propose to add a chunked version for `bulk`.
This allows implementations to process the iteration space in chunks, and take advantage of the locality of the chunk processing.
This is useful when publishing the effects of the functor may be expensive (the example from the motivation) and when the given functor cannot be inlined.

The implementation of `bulk_chunked` can use dynamically sized chunks that adapt to the workload at hand.
For example, computing the results of a Mandelbrot fractal on a line is an unbalanced process.
Some values can be computed very fast, while others take longer to compute.

Passing a range as parameters to the functor passed to `bulk_chunked` is similar to the way Intel TBB's `parallel_for` functions (see [[parallel_for]]).

An implementation of `bulk` can be easily built on top of `bulk_chunked` without losing performance (as shown in the Proposal section).


## Also define an unchunked version ## {#design.unchunked}

[[P3564R0]] explains the necessity
of having a version of `bulk` that executes each loop iteration
on a distinct execution agent:

1. For schedulers that promise concurrent forward progress, users need a way to take advantage of concurrent forward
     progress. Since `bulk` execution can do chunking, it cannot promise anything stronger than parallel forward progress.
1. Even for schedulers with a weaker forward progress guarantee, users sometimes have performance reasons
     to control the distribution of loop iterations to execution agents.

Revisions R2 and R3 originally incorporated this design suggestion from [[P3564R0]] into this paper and name this API `bulk_unchunked`.
After further discussions in Sofia 2025, we realized that these two directions do not overlap well, and that it likely deserve separate APIs for the two design aspects.
We decided that we can have `bulk_unchunked` as an algorithm to be used when the user wants to execute each iteration on a distinct execution agent, and a new algorithm `bulk_concurrent` that explicitly deals with concurrent forward progress guarantee.

This paper proposes `bulk_unchunked`, but does not propose `bulk_concurrent`, leaving this for future work.

## Using execution policies ## {#design.policies}

As discussed in the [[#motivation]] section, not having the possibility to specify execution policies for the `bulk` algorithm is a downside.
On the one hand, defaulting to any policy other than `seq` might lead to deadlocks.
On the other hand, defaulting to `seq` would not match users' expectations
that `bulk` would take advantage of parallel execution resources.
Thus, `bulk` has to have an execution policy parameter.

One criticism of this solution is that each invocation of `bulk` needs to contain an extra parameter that is typically verbose to type.

The idea considered by the authors to solve it is to provide versions of the algorithms that have the execution policies
already baked in.
Something like:

```cpp
auto bulk_seq(auto prev, auto size, auto f);
auto bulk_unseq(auto prev, auto size, auto f);
auto bulk_par(auto prev, auto size, auto f);
auto bulk_par_unseq(auto prev, auto size, auto f);

auto bulk_chunked_seq(auto prev, auto size, auto f);
auto bulk_chunked_unseq(auto prev, auto size, auto f);
auto bulk_chunked_par(auto prev, auto size, auto f);
auto bulk_chunked_par_unseq(auto prev, auto size, auto f);
```

We dropped this idea, as this isn't scalable.

On the other hand, it's quite easy to write a thin wrapper on top of `bulk` / `bulk_chunked` / `bulk_unchunked` on the user side that calls the algorithm
with the execution policy of choice.
Something like:

```cpp
auto user_bulk_par(auto prev, auto size, auto f) {
  return std::execution::bulk(std::execution::par, prev, size, f);
}
```

## Conflicting execution policies ## {#design.conflict.policies}

For [[P2500R2]] design we previously agreed that there might be an execution policy attached to a `scheduler`. The
question that comes to mind is what should we do if we have execution policy passed via `bulk` parameters that conflicts with
execution policy attached to a scheduler?

This is another area to explore. It's quite obvious that execution policy passed to `bulk` describes possible parallelization
ways of `bulk` callable object, while it's not 100% obvious what execution policy attached to a scheduler means outside of
[[P2500R2]] proposal.

We might choose different strategies (not all of them are mutually exclusive):
- Reconsider the decisions we made for [[P2500R2]] by stopping attaching policies to schedulers.
- Reduce policies: choose the most conservative one. For example, for the passed policy `par` and attached policy `seq` we
    reduce it the resulting policy to `seq` because it's the most conservative one.
    - For "peer" conflicting policies: `par` and `unseq` we might either want to reduce it to `seq` or might want to give a
        compile-time error.
- Give a compile-time error right away for any pair of conflicting policies.

Anyway, exploration of this problem is out of scope of this paper.
For the purpose of this paper, we need to ensure that the way the given function is called is compatible with the execution policy passed to `bulk`.

## Expectations on customizations ## {#design.expectations}

The current specification of `bulk` in [[exec.bulk]] doesn't define any constraints/expectations for the customization of the algorithm.
The default implementation (a serial version of `bulk`) is expected to be very different that its customizations.

Having customizations in mind, we need to define the minimal expectations for calling `bulk`.
We propose the following:
- Mandate `f` to be copy-constructible.
- Allow `f` to be invoked according to the specified execution policy (and don't assume it's going to be called serially).
- Allow the values produced by the previous sender to be decay copied (if the values produced are copyable).
- Allow the algorithm to handle cancellation requests, but don't mandate it.


We want to require that the given functor be copy-constructible so that `bulk` requires the same type of function as a `for_each` with an execution policy.
Adding an execution policy to `bulk`/`bulk_chunked` would also align them with `for_each`.

If `f` is allowed to be invoked concurrently, then implementations may want to copy the arguments passed to it.

Another important point for a `bulk` operation is how it should react to cancellation requests.
We want to specify that `bulk` should be able to react to cancellation requests, but not make this mandatory.
It shall be also possible for vendors to completely ignore cancellation requests (if, for example, supporting cancellation would actually slow down the main uses-cases that the vendor is optimizing for).
Also, the way cancellation is implemented should be left unspecified.

Checking the cancellation token every iteration may be expensive for certain cases.
A cancellation check requires an acquire memory barrier (which may be too much for really small functors), and might prevent vectorization.
Thus, implementations may want to check the cancellation every *N* iterations; *N* can be a statically known number, or can be dynamically determined.

As `bulk`, `bulk_chunked`, and `bulk_unchunked` are customization points, we need to specify these constraints to all three of them.


## Exception handling ## {#design.exceptions}

While there are multiple solutions to specify which errors can be thrown by `bulk`, the most sensible ones seem to be:
1. pick an arbitrary exception thrown by one of the invocations of `f` (maybe using a atomic operations to select the first thrown exception),

2. reduce the exceptions to another exception that can carry one or more of the thrown exceptions using a user-defined reduction operation (similar to using `exception_list` as discussed by [[P0333R0]]),

3. allow implementations to produce a new exception type (e.g., to represent failures outside of `f`, or to represent failure to transmit the original exception to the receiver)

One should note that option 2 can be seen as a particular case of option 3.

Also, option 2 seems to be more complex than option 1, without providing any palpable benefits to the users.

The third option is very useful when implementations may fail outside of the calls to the given functor.
Also, there may be cases in which exceptions cannot be transported from the place they were thrown to the place they need to be reported.

Based on the experience with the existing parallel frameworks we incline to recommend the option one because
-	In a parallel execution the common behavior is non-deterministic by nature.
    Thus, we can peak arbitrary exception to throw

-	Catching any exception already indicates that something went wrong, thus a good implementation might initiate a cancellation mechanism to finish already failed work as soon as possible.

On top of that, for special cases we also want to allow option 3.


# Specification # {#specification}

## In [execution.syn] ## {#specification.syn}

<pre highlight="c++">
  struct let_value_t { unspecified };
  struct let_error_t { unspecified };
  struct let_stopped_t { unspecified };
  struct bulk_t { unspecified };
</pre>
<div class="block-insert">
<pre highlight="c++">
  struct bulk_chunked_t { unspecified };
  struct bulk_unchunked_t { unspecified };
</pre>
</div>
[...]

<pre highlight="c++">
  inline constexpr let_value_t let_value{};
  inline constexpr let_error_t let_error{};
  inline constexpr let_stopped_t let_stopped{};
  inline constexpr bulk_t bulk{};
</pre>
<div class="block-insert">
<pre highlight="c++">
  inline constexpr bulk_chunked_t bulk_chunked{};
  inline constexpr bulk_unchunked_t bulk_unchunked{};
</pre>
</div>

## In [exec.bulk] ## {#specification.bulk}

<span class="ed-note">Rename the title of the section from “execution::bulk” to “execution::bulk, execution::bulk_chunked, and execution::bulk_unchunked”.</span>

<span class="ed-note">Apply the following changes (no track changes for paragraph numbering):</span>

1. `bulk`<ins>, `bulk_chunked`, and `bulk_unchunked`</ins> run<del>s</del> a task repeatedly for every index in an index space.

2. The <del>name</del><ins>names</ins> `bulk`<ins>, `bulk_chunked`, and `bulk_unchunked`</ins> <del>denotes a</del><ins>denote</ins> pipeable sender adaptor <del>object</del><ins>objects</ins>.<ins>
    Let <i>`bulk-algo`</i> be either `bulk`, `bulk_chunked`, or `bulk_unchunked`.</ins>
    For subexpressions `sndr`, <ins>`policy`,</ins> `shape`, and `f`, let <ins>`Policy` be `remove_cvref_t<decltype(policy)>`,</ins> `Shape` be `decltype(auto(shape))` <ins>, and `Func` be `decay_t<decltype((f))>`</ins>.
    If

      - `decltype((sndr))` does not satisfy `sender`, or
      - <ins>`is_execution_policy_v<Policy>` is `false`, or</ins>
      - `Shape` does not satisfy `integral`, or
      - <del>`decltype((f))`</del><ins>`Func`</ins> does not <del>satisfy <i>`movable-value`</i></del> <ins>model `copy_constructible`</ins>,

    <del>bulk(sndr, shape, f)</del><ins><i>`bulk-algo`</i>`(sndr, policy, shape, f)`</ins> is ill-formed.

3. Otherwise, the expression <del>`bulk(sndr, shape, f)`</del><ins><i>`bulk-algo`</i>`(sndr, policy, shape, f)`</ins> is expression-equivalent to:

        <pre highlight="c++">
        transform_sender(
          <i>get-domain-early</i>(sndr),
          <i>make-sender</i>(<del>bulk</del><ins><i>bulk-algo</i></ins>, <i>product-type</i><ins>&lt;<i>see below</i>, Shape, Func></ins>{<ins>policy, </ins>shape, f}, sndr))
        </pre>


    except that `sndr` is evaluated only once.

    <div class="block-insert">
    3.1. The first template argument of <i>`product-type`</i> is `Policy` if `Policy` models `copy_constructible`, and `const Policy&` otherwise.
    </div>

<div class="block-insert">

4. Let `sndr` and `env` be subexpressions such that `Sndr` is `decltype((sndr))`. If <i>`sender-for`</i>`<Sndr, bulk_t>` is `false`, then the expression `bulk.transform_sender(sndr, env)` is ill-formed; otherwise, it is equivalent to:

        <pre highlight="c++">
        auto [_, data, child] = sndr;
        auto& [policy, shape, f] = data;
        auto new_f = [func=std::move(f)](Shape begin, Shape end, auto&&... vs)
            noexcept(noexcept(f(begin, vs...))) {
          while (begin != end) func(begin++, vs...);
        }
        return bulk_chunked(std::move(child), policy, shape, std::move(new_f));
        </pre>

    <i>[Note ?</i>: This causes the `bulk(sndr, policy, shape, f)` sender to be expressed in terms of `bulk_chunked(sndr, policy, shape, f)` when it is connected to a receiver whose execution domain does not customize `bulk`. <i> — end note]</i>

</div>

<div class="block-insert">
5. The exposition-only class template `impls-for` ([exec.snd.general]) is specialized for `bulk_chunked_t` as follows:

        <pre highlight="c++">
        namespace std::execution {
          template&lt;>
          struct <i>impls-for</i>&lt;bulk_chunked_t> : <i>default-impls</i> {
            static constexpr auto <i>complete</i> = <i>see below</i>;
          };
        }
        </pre>

    5.1. The member <code><i>impls-for</i>&lt;bulk_chunked_t>::<i>complete</i></code> is initialized with a callable object equivalent to the following lambda:

          <pre highlight="c++">
          []&lt;class Index, class State, class Rcvr, class Tag, class... Args>
            (Index, State& state, Rcvr& rcvr, Tag, Args&&... args) noexcept
            -> void requires <i>see below</i> {
              if constexpr (same_as&lt;Tag, set_value_t>) {
                auto& [policy, shape, f] = state;
                constexpr bool nothrow = noexcept(f(auto(shape), auto(shape), args...));
                <i>TRY-EVAL</i>(rcvr, [&]() noexcept(nothrow) {
                  f(static_cast&lt;decltype(auto(shape))>(0), auto(shape), args...);
                  Tag()(std::move(rcvr), std::forward&lt;Args>(args)...);
                }());
              } else {
                Tag()(std::move(rcvr), std::forward&lt;Args>(args)...);
              }
            }
          </pre>

          5.1.1. The expression in the *requires-clause* of the lambda above is `true` if and only if `Tag` denotes a type other than `set_value_t` or if the expression `f(auto(shape), auto(shape), args...)` is well-formed.

</div>

6. The exposition-only class template <i>`impls-for`</i> ([exec.snd.general]) is specialized for <del>`bulk_t`</del><ins>`bulk_unchunked_t`</ins> as follows:

        <pre highlight="c++">
        namespace std::execution {
          template&lt;>
          struct <i>impls-for</i>&lt;<del>bulk_t</del><ins>bulk_unchunked_t</ins>> : <i>default-impls</i> {
            static constexpr auto <i>complete</i> = <i>see below</i>;
          };
        }
        </pre>

    6.1. The member <code><i>impls-for</i>&lt;<del>bulk_t</del><ins>bulk_unchunked_t</ins>>::<i>complete</i></code> is initialized with a callable object equivalent to the following lambda:

          <pre highlight="c++">
          []&lt;class Index, class State, class Rcvr, class Tag, class... Args>
            (Index, State& state, Rcvr& rcvr, Tag, Args&&... args) noexcept
            -> void requires <i>see below</i> {
              if constexpr (same_as&lt;Tag, set_value_t>) {
                auto& [shape, f] = state;
                constexpr bool nothrow = noexcept(f(auto(shape), args...));
                <i>TRY-EVAL</i>(rcvr, [&]() noexcept(nothrow) {
                  for (decltype(auto(shape)) i = 0; i < shape; ++i) {
                    f(auto(i), args...);
                  }
                  Tag()(std::move(rcvr), std::forward&lt;Args>(args)...);
                }());
              } else {
                Tag()(std::move(rcvr), std::forward&lt;Args>(args)...);
              }
            }
          </pre>

          6.1.1. The expression in the *requires-clause* of the lambda above is `true` if and only if `Tag` denotes a type other than `set_value_t` or if the expression `f(auto(shape), args...)` is well-formed.

7. Let the subexpression `out_sndr` denote the result of the invocation <del>`bulk`</del><ins><i>`bulk-algo`</i></ins>`(sndr, `<ins>`policy,`</ins>` shape, f)` or an object equal to such, and let the subexpression `rcvr` denote a receiver such that the expression `connect(out_sndr, rcvr)` is well-formed.
    The expression `connect(out_sndr, rcvr)` has undefined behavior unless it creates an asynchronous operation ([async.ops]) that, when started:

      - <del>on a value completion operation, invokes `f(i, args...)` for every `i` of type `Shape` from `0` to `shape`, where `args` is a pack of lvalue subexpressions referring to the value completion result datums of the input sender, and</del>

      - <del>propagates all completion operations sent by `sndr`.<del>

      <ins>
      - If `sndr` has a successful completion, where `args` is a pack of lvalue subexpressions referring to the value completion result datums of `sndr`, or decayed copies of those values if they model `copy_constructible`, then:

        - If `out_sndr` also completes successfully, then:

            - for `bulk`, invokes `f(i, args...)` for every `i` of type `Shape` from `0` to `shape`;

            - for `bulk_unchunked`, invokes `f(i, args...)` for every `i` of type `Shape` from `0` to `shape`;

                - <i>Recommended practice: </i> The underlying scheduler should execute each iteration on a distinct execution agent.

            - for `bulk_chunked`, invokes `f(b, e, args...)` zero or more times with pairs of `b` and `e` of type `Shape` in range `[0, shape]`, such that `b < e` and for every `i` of type `Shape` from `0` to `shape`, there is exactly one invocation with a pair `b` and `e`, such that `i` is in the range `[b, e)`.


        - If `out_sndr` completes with `set_error(rcvr, eptr)`, then the asynchronous operation may invoke a subset of the invocations of `f` before the error completion handler is called, and `eptr` is an `exception_ptr` containing  either:

            - an exception thrown by an invocation of `f`, or

            - a `bad_alloc` exception if the implementation fails to allocate required resources, or

            - an exception derived from `runtime_error`.

        - If `out_sndr` completes with `set_stopped(rcvr)`, then the asynchronous operation may invoke a subset of the invocations of `f` before the stopped completion handler.

      - If `sndr` does not complete with `set_value`, then the completion is forwarded to `recv`.

      - For <i>`bulk-algo`</i>, the parameter `policy` describes the manner in which the execution of the asynchronous operations corresponding to these algorithms may be parallelized and the manner in which they apply `f`. Permissions and requirements on parallel algorithm element access functions ([algorithms.parallel.exec]) apply to `f`.

            </ins>

<div class="block-insert">
8. <i>[Note: </i> The asynchronous operation corresponding to <i>`bulk-algo`</i>`(sndr, policy, shape, f)` can complete with `set_stopped` if cancellation is requested or ignore cancellation requests. <i> — end note]</i>


</div>


# Acknowledgements # {#acknowledgements}

Special thanks to Christian Trott and Tom Scogland for a very productive discussion on `bulk_unchunked` and help on working improvements.

# Polls # {#polls}

## SG1, Hagenberg, Austria, 2025 ## {#sg1.austria.2025}

Summary: Forward P3481R1 with the following notes:

    - `bulk_unchunked` should not have an execution policy
    - Calling an un-customized `bulk_unchunked` is ill-formed on a concurrent scheduler
    - The next revision of this paper (before LWG) needs wording for its error model

    ```
    | SF | F | N | A | SA |
    | 8  | 2 | 2 | 0 | 0  |
    ```
    Consensus

## SG1, Wrocław, Poland, 2024 ## {#sg1.poland.2024}

[[P3481R0]] was presented at the SG1 meeting in November 2024 in Wrocław, Poland.
SG1 provided the following feedback in the form of polls:

1. If we have chunking, we need to expose a control on the chunk size.

    ```
    | SF | F | N | A | SA |
    | 1  | 2 | 3 | 1 | 1  |
    ```

    No consensus


2. We need a version of the `bulk` API that presents the chunk to the callable in order to implement the parallel algorithms

    ```
    | SF | F | N | A | SA |
    | 4  | 2 | 1 | 0 | 1  |
    ```
    Consensus in favor

3. We need a version of the bulk API that creates an execution agent per iteration.

    ```
    | SF | F | N | A | SA |
    | 2  | 3 | 3 | 0 | 0  |
    ```

    Unanimous consent

4. We believe / desire:
    1. Bulk needs an execution policy to describe the callable, IF the scheduler also has an execution
        policy (for debugging for example) then a conservative choice should be used (`seq` is more
        conservative than `par`)
    2. No SG1 concerns with the proposed exception handling
    3. No change to status quo on default implementation of bulk being serial
    4. No change to status quo on bulk having a predecessor
    5. Forms of bulk needed:
        - `bulk` -> `f(i)` -> `bulk_chunked`
        - `bulk_chunked` -> `f(b, e)`
        - `bulk_unchunked` -> `f(i)` "execution agent per iteration"

    ```
    | SF | F | N | A | SA |
    | 5  | 2 | 1 | 0 | 0  |
    ```

    Unanimous consent


<pre class=biblio>
{
    "exec.bulk": {
        "authors": ["ISO WG21"],
        "title": "Working Draft: Programming Languages — C++ -- `execution::bulk`",
        "publisher": "ISO",
        "href": "https://eel.is/c++draft/exec.bulk"
    },
    "alg.foreach": {
        "authors": ["ISO WG21"],
        "title": "Working Draft: Programming Languages — C++ -- For each",
        "publisher": "ISO",
        "href": "https://eel.is/c++draft/alg.foreach"
    },
    "parallel_for": {
        "authors": ["Intel"],
        "title": "Intel® oneAPI Threading Building Blocks Developer Guide and API Reference -- parallel_for",
        "href": "https://www.intel.com/content/www/us/en/docs/onetbb/developer-guide-api-reference/2021-6/parallel-for.html"
    }
}
</pre>

